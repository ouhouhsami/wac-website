<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <script type="text/javascript" src="https://code.jquery.com/jquery-2.1.3.min.js"></script>
    <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro' rel='stylesheet' type='text/css'>
    <style>
      html {
        font-size: 64.5%;
        font-family: "Source Sans Pro";
        -webkit-text-size-adjust: none;
      }

      body {
        margin: 0;
        -webkit-text-size-adjust: none;
      }

      h1 {
        font-size: 3rem;
        margin: 0;
        padding: 20px;
        color: rgba(0, 0, 0, 0.8);
      }
      h2 {
        margin: 0;
        padding: 20px;
        color: rgba(0, 0, 0, 0.8);
      }

      table {
        border-collapse:collapse;
        border: none;
        width: 100%;
        margin-bottom: 20px;
      }

      th {
        border: none;
      }

      .no-border { border: none; }

      td {
        border: none;
        border-top: 1px dotted #dedede;
        font-size: 1.4rem;
        padding: 4px;
      }

      .category td:nth-child(1) {
        background-color: #efefef;
        font-size: 1.8rem;
        color: #EA6353;
      }

      .category td:nth-child(2) {
        background-color: #efefef;
        font-size: 1.8rem;
        padding-left: 0px;
        color: #EA6353;
      }

      tr:nth-child(2) td {
        border: none;
      }

      td:nth-child(1) {
        width: 20%;
        padding-right: 30px;
        text-align: right;
        font-size: 1.6rem;
        color: #787878;
        vertical-align: top;
      }

      td:nth-child(2) {
        padding-left: 8px;
      }

      a {
        color:#EA6353;
      }

      p {
        margin: 14px 14px 14px 0;
        font-style: italic;
        color: #565656;
      }
/*
      p:before {
        content: '"';
      }

      p:after {
        content: '"';
      }
  */
      .hide {
        display: none;
      }

      .day th {
        background-color: #545454;
        font-size: 1.8rem;
        color: rgba(255, 255, 255, 0.9);
        padding: 12px;
      }

      .half-day {
        background-color: #DCDCDC;
        font-size: 1.1rem;
      }

      .title:hover {
        cursor: pointer;
        color:#EA6353;
      }

      @media only screen and (max-width: 800px) {
        h1 {
          font-size: 2.4rem;
          padding: 20px 12px;
        }

        td:nth-child(1) {
          font-size: 1.4rem;
          padding-right: 10px;
        }

        .category td:nth-child(1) {
          font-size: 1.4rem;
          text-align: center;
          padding: 0;
          vertical-align: middle;
        }

        .category tr:nth-child(1) td {
          text-align: center;
        }
      }
    </style>
    <script>
      $( document ).ready(function() {
        var $titleTd = $('.title');
        $titleTd.next().addClass('abstract hide');
        $titleTd.on('click', function(){
          $(this).next().toggleClass('hide');
        })
      });
    </script>
  </head>
  <body>
    <h1>WAC 2015 - Program</h1>
    <h2>
      The abstracts of both, Web Audio Talks and paper presentations, are accessible by clicking on the title below.
      <br />
      The full papers are accessible via the PDF link.
    </h2>
    <table>
      <tr class="day">
        <th colspan=2>
          Monday morning, January 26, 2015 - IRCAM - Igor Stravinsky Room
        </th>
      </tr>
      <tr class="category">
        <td>
          8.00
        </td>
        <td>
          Welcome - Lobby
        </td>
      </tr>
      <tr>
        <td>
          9.00
        </td>
        <td>
          <div><i>WAC Introduction</i> - Hugues Vinet, Samuel Goldszmidt, and Norbert Schnell </div>
        </td>
      </tr>
      <tr>
        <td>
          9.15
        </td>
        <td>
          <div class="title"><i>Keynote #1 Audio and the Web</i> - Chris Wilson</div>
          <div>
            <p>
              The web has supported multiple media since its inception - however, only
              recently has it become a viable platform for building audio applications.
              The talk will examine the journey of audio in the web platform, the
              intersection of interesting technologies that make this a pivotal point for
              audio and the web, and will highlight the opportunities unlocked by web
              audio and where we go from here.
            <br />
              Chris Wilson is a Developer Advocate on the Google Chrome team.  He started
              working on web browsers in 1993 when he co-authored the original Windows
              version of NCSA Mosaic before working on Internet Explorer for fifteen
              years at Microsoft.  He has separate and combined passions for digital
              audio, music and the web, and co-edits the Web Audio and Web MIDI
              specifications at the W3C.  He also specializes in playing many different
              instruments badly.
            </p>
          </div>
        </td>
      </tr>
      <tr class="category">
        <td>
          10.00
        </td>
        <td>
          Coffee Break (gallery level -2)
        </td>
      </tr>
      <tr class="category">
        <td>
          <b>10.30 - 13.00</b>
        </td>
        <td>
          <b>Tools & Components</b> | <small>Session Moderator: Raphaël Troncy</small>
        </td>
      </tr>
      <tr>
        <td>
          10.30
        </td>
        <td>
          <div class="title"><i>Building a Collaborative Digital Audio Workstation Based on the Web Audio API</i> - Jan Monschke</div>
          <div>
            <p>
              The introduction of the Web Audio API has enriched the web landscape enormously. It gives game developers the ability to add precisely timed, high performant sound effects and to create realistic spatialized sound landscapes.
            <br />
              For many web developers it is the first time they encounter audio programming which leads to many interesting experiments when the world of web and audio collide. Traditional web developers start to become really interested in audio programming and educate themselves on the topic of synthesizers and audio effects.
            <br />
              However, currently there are only few applications that try to create more sophisticates audio production tools in the browser. In this talk I want to show how I used the Web Audio API and other emerging web standards like WebRTC to create a collaborative digital audio workstation. It allows users to record, arrange and create their own songs in collaboration with other users in real time.
            </p>
            <a href="http://web-audio-editor.herokuapp.com/">http://web-audio-editor.herokuapp.com/</a>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          10.50
        </td>
        <td>
          <div class="title"><i>DAW Plugins for Web Browsers</i> - Jari Kleimola - <a href="pdf/wac15_submission_23.pdf">PDF</a></div>
          <div>
            <p>
              A large collection of Digital Audio Workstation (DAW) plugins is available on the Internet in open source form. This paper explores their reuse in browser environments, focusing on hosting options that do not require manual installation. Two options based on Emscripten and PNaCl are introduced, implemented, evaluated, and released as open source. We found that ported DAW effect and sound synthesizer plugins complement and integrate with the Web Audio API, and that the existing preset patch collections make the plugins readily usable in online contexts. The latency measures are higher than in native plugin implementations, but expected to reduce with the emerging AudioWorker node.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          11.10
        </td>
        <td>
          <div class="title"><i>Meyda: an Audio Feature Extraction Library for the Web Audio API</i> - Hugh Rawlinson, Nevo
          Segal, and Jakub Fiala - <a href="pdf/wac15_submission_17.pdf">PDF</a></div>
          <div>
            <p>
              There are many existing native libraries and frameworks for audio feature extraction used in multimedia information retrieval. Many are dependent on highly optimised low level code to cope with the high performance requirements of realtime audio analysis. In this paper, we present a new audio feature extractor library, Meyda, for use with the JavaScript Web Audio API, and detail its benchmarking results. Meyda provides the first library for audio feature extraction in the web client, which will enable music information retrieval systems, complex visualisations and a wide variety of technologies and creative projects that previously were relegated to native software. The Meyda project, including source code and documentation is released under an MIT license.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          11.30
        </td>
        <td>
          <div class="title"><i>Web Audio Tools</i> - Jordan Santell</div>
          <div>
            <p>
              As technologies evolve and mature, proper tooling is needed to increase adoption and combat complexity. Web audio is no exception.
            <br />
              An overture on the state of web audio tools, we will explore using both browser developer tools that take advantage of privileged platform code, and cross-browser drop-in libraries to provide introspection of the state of an audio context and nodes. With these tools, we’ll cover common debugging scenarios like audio graph construction, node-to-node signal transformations, signal visualizations, garbage collection, and resource consumption. These tools can help beginners understand a complex new API, save a developer’s time debugging, and help browser implementers optimize their platform.
              Attendees will leave with the knowledge of what tools are out there, workflows for inspecting, fixing, and optimizing a web audio environment, and how to make their own tools for a specific use case.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          11.50
        </td>
        <td>
          <div class="title"><i>Adventures in Scheduling, Buffers and Parameters: Porting a Dynamic Audio Engine to Web Audio</i> - Chinmay Pendharkar, Peter Bäck, and Lonce Wyse - <a href="pdf/wac15_submission_12.pdf">PDF</a></div>
          <div>
            <p>
              At Sonoport, we ported our Dynamic Sound Engine from Adobe's Flash technology to Web Audio API. The difference in approaches to threading, scheduling, and parameters between Flash and Web Audio API created a few challenges for us. These differences and some peculiarities of the Web Audio API required workarounds to be able to implement our Dynamic Sound Engine in Web Audio API. In this paper we discuss three of these workarounds dealing with creating parameters, scheduling operations, and playback position of buffers, and explain how these work-arounds, although not optimal solutions, allowed us to support our use cases. Finally, we consider how the upcoming AudioWorker change in the Web Audio API specification is expected to impact these workarounds.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          12.10
        </td>
        <td>
          <div class="title"><i>Audio Oriented UI Components for the Web Platform</i> - Victor Saiz,
          Benjamin Matuszewski, and Samuel Goldszmidt - <a href="pdf/wac15_submission_27.pdf">PDF</a></div>
          <div>
            <p>
              This paper presents a set of web-native tools for visualis- ing and interacting with time-based objects. These visu- alisations are rendered as part of the document using web standard technologies, allowing for an easy integration and interaction with the elements on the same document with- out the help of any non-native technologies such as Adobe Flash, Microsoft’s Silverlight or Oracle’s Java.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          12.30
        </td>
        <td>
          <div class="title"><i>Of Time Engines and Masters — An API for Scheduling and Synchronizing the Generation and Playback of Event Sequences and Media Streams for the Web Audio API</i> - Norbert Schnell, Victor Saiz, Karim Barkati,
          and Samuel Goldszmidt - <a href="pdf/wac15_submission_19.pdf">PDF</a></div>
          <div>
            <p>
              In this article we present an API and a set of JavaScript modules for the synchronized scheduling and aligned playback of predetermined sequences of events such as notes, audio segments, and parameter changes as well as media streams (e.g. audio buffers) based on the Web Audio API logical time.
            <br />
              The API has been designed to facilitate the development on both ends, the implementation of modules which generate event sequences or media streams as well as the integration of such modules into complex audio applications that require flexible scheduling, playback and synchronization.
            </p>
          </div>
        </td>
      </tr>
      <tr class="category">
        <td>
          13.00
        </td>
        <td>
          Lunch (Gallery, Level -2)
        </td>
      </tr>
    </table>
    <table>
      <tr class="day">
        <th colspan=2>
          Monday afternoon, January 26, 2015 - IRCAM - Igor Stravinsky Room
        </th>
      </tr>
      <tr>
        <td>
          14.00
        </td>
        <td>
          <div class="title"><i>Keynote #2 The First Computer Music Programming Language</i> - Chris Lowis</div>
          <div>
            <p>
              MUSIC was a programming language developed by Max Mathews at Bell Labs in 1957. In this talk we'll learn more about Max Mathews, the origins of computer music, and by building a compiler for MUSIC in JavaScript hear what some of the very first computer music compositions sounded like.
            <br />
              Chris Lowis is an invited expert on the W3C's Audio Working
              group. He studied acoustics and signal processing at the Institute of Sound
              and Vibration Research in Southampton, and recently worked at the R&D
              department at the BBC.  He loves to use the Web Audio API to bring old
              synthesisers back to life, and to write about audio on the web in his
              newsletter Web Audio Weekly.
            </p>
          </div>
        </td>
      </tr>
      <tr class="category">
        <td>
          <b>14.45 - 16.15</b>
        </td>
        <td>
          <b>Languages & Environments</b> | <small>Session Moderator: Matthew Paradis</small>
        </td>
      </tr>
      <tr>
        <td>
          14.45
        </td>
        <td>
          <div class="title"><i>Can Web Audio be Liberated from the Von Neumann Style?</i> - Emilio Jesús Gallego Arias</div>
          <div>
            <p>
              The audio programming world is extremely prolific in domain-specific languages.
              Programmers and musicians prefer a custom-tailored environment where their high-level ideas can be better expressed and implemented thanks to specific domain knowledge.
            <br />
              In particular, strongly typed functional programming has become an ideal platform for audio programming, thanks to a concise, high-level writing style, high reusability, and ability to avoid mistakes and improve error handling.
            <br />
              - How may Web Audio benefit from these ideas?
            <br />
              - How may such languages benefit from Web Audio?
            <br />
              In this talk, we will use the functional audio programming language Faust as the reference DSL, to discuss a formal theory of interoperable and efficient audio components. Particular emphasis will be made on the use of strong, functional-style type-systems.
            <br />
              We will address performance, security, and practical considerations, and reflect on the relationship of such theoretical framework to the actual standard.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          15.05
        </td>
        <td>
          <div class="title"><i>Extending Csound to the Web</i> - Victor Lazzarini,
          Edward Costello, Steven Yi, and John Ffitch - <a href="pdf/wac15_submission_14.pdf">PDF</a></div>
          <div>
            <p>
              This paper discusses the presence of the sound and music computing system Csound in the modern world-wide web browser platform. It introduces the two versions of the system currently available, as pure Javascript code, and as portable Native Client binary module with a Javascript interface. Three example applications are presented, showing some of the potential uses of the system. The paper concludes with a discussion of the wider Csound application ecosystem, and the prospects for its future development.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          15.25
        </td>
        <td>
          <div class="title"><i>BRAID: A Web Audio Instrument Builder with Embedded Code Blocks</i> - Benjamin Taylor and Jesse Allison - <a href="pdf/wac15_submission_33.pdf">PDF</a></div>
          <div>
            <p>
              Braid (Browser Audio Interface and Database) is a web audio instrument-building environment developed with the NexusUI platform. To identify the requirements of such an environment, the utility of NexusUI as an audio interface engine for web browser-based projects is reviewed. The addition of inline web audio within a drag-and-drop interface-building environment is discussed. A consideration of a modified Model-View-Controller architecture to integrate DSP code and interface is followed by an examination of the workflow of designing browser-based instruments within Braid. Finally, a database for saving and sharing web audio instruments for performance or audience distribution is described.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          15.45
        </td>
        <td>
          <div class="title"><i>Interactive Music with Tone.js</i> - Yotam Mann - <a href="pdf/wac15_submission_40.pdf">PDF</a></div>
          <div>
            <p>
              This paper discusses the features, architecture and implementation of Tone.js, a Web Audio framework to facilitate the creation of interactive music specifically suited to the affordances of the browser.
            </p>
          </div>
        </td>
      </tr>
      <tr class="category">
        <td>
          <b>16.15</b>
        </td>
        <td>
          <b>Demo / Poster Session #1 & Coffee Break (Gallery, Level -2)</b>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>WAVE Project Demo and Enhanced Published Score</i> -  Benjamin Matuszewski
          and Samuel Goldszmidt</div>
          <div>
The presentation will focus on how emerging web standards can provide
score based tools for educational and musicological purposes.
<br />
Among others, SVG, Web Audio API, javascript libraries (d3, etc.), offer
new ways to consider and extend an already edited and commonly available
score: the ""pdf"" version provided by publishers. Moreover published
scores often contain specific notation idioms for contemporary music,
not easily available in digital notation formats.
<br />
While working on raw images of the score, an application can enhance it
with metadata like shape and text annotations. We can also consider, in
conjunction with the Web Audio API, to synchronise a score and its
performances, further allowing to highlight some aspects of the
composition by audio manipulation.
<br />
We will show demos of these approaches within different applications
based on Webern Opus 27, Bach first Prelude of the Well-Tempered Clavier.
</div>

        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Noteflight: A Web-standards-based Compositional Community</i> - Joseph Berkovitz</div>
          <i
            ><a href="http://www.noteflight.com">http://www.noteflight.com</a></i>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Web-Based Visualizations and Acoustic Rendering for Multimodal Data from Orchestra Performances using Repovizz </i> - Oscar Mayor</div>
          <div>
            In the demo a set of fully working web-based prototypes developed in the
context of the EU FP7 PHENICX Project (http://phenicx.upf.edu) will be
presented. The Phenicx project is about innovating the classical music
experience providing them with a multimodal, multi-perspective and
multilayer interactive engagement, before, during and after the concert.
In this demo we present some prototypes that are related with the post
concert experience.
<br />
We have recorded a set of classical pieces performed by top level
orchestras, including some data modalities like multi-channel audio,
video, motion capture sensors, midi and text. Once all data streams have
been time-synchronized, we have performed the following analysis on the
data:
<br />
- Low-level and high-level audio descriptors for each individual audio
source<br />
- Description of conductor gestures based on the motion capture sensors
- Score to performance alignment<br />
- Audio source separation<br />
- Musical structure analysis of the performed piece
<br />
Then all these data is uploaded to the repovizz web repository
(repovizz.upf.edu) that allows visualization and sharing of the data
over the network. A set of customized web-based visualizations have been
designed to build the prototypes that will be shown in this demo.
Multimodal data streams are accessed on-line using the repovizz web API
and html5 is used for the visualizations of the multimodal data and
descriptors extracted from the performances. The web-audio API is used
to handle the audio rendering in the client to mix between the different
audio channels obtained from the different recorded audio sources or
from the automatic isolation of instruments performed in the analysis step.
<br />
The visualizations available include (all web-based):
- Scrolling piano roll visualization of the musical score while audio is
playing
- Orchestra layout visualization showing instrument activity and
loudness while playing audio<br />
- Audio focus to hear individual instruments playing alone<br />
- Multi-perspective video angle selection during the concert<br />
- 3D render of the conductor body
<br />
Here is a live example of the orchestra layout visualization including
the isolation of instruments as an example of one of the prototypes that
will be shown during the demo:
http://repovizz.upf.edu/phenicx/RCO-Eroica/
<br />
Video showing some of the repovizz orchestra visualizations developed in
the context of PHENICX:
https://www.youtube.com/watch?v=c7pmDvkKY7A#t=168
          </div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Repovizz - Multimodal Online Database and Visualization Tool</i> - Quim Llimona, Oscar Mayor, Esteban Maestre, Panos Papiotis
</div>
          <div>
        Repovizz is an integrated online system capable of structural formatting and remote storage, browsing, exchange, annotation, and visualization of
synchronous multi-modal, time-aligned data. Motivated by a growing need
for data-driven collaborative research, repovizz aims to resolve
commonly encountered difficulties in sharing or browsing large
collections of multi-modal data. At its current state, repovizz is
designed to hold time-aligned streams of heterogeneous data: audio,
video, motion capture, physiological signals, extracted descriptors,
annotations, et cetera. Most popular formats for audio and video are
supported, while CSV-based formats are adopted for streams other than
audio or video (e.g., motion capture or physiological signals). The data
itself is structured via customized XML files, allowing the user to
(re-) organize multi-modal data in any hierarchical manner, as the XML
structure only holds metadata and pointers to data files. Datasets are
stored in an online database, allowing the user to interact with the
data remotely through a powerful HTML5 visual interface accessible from
any standard web browser; this feature can be considered a key aspect of
repovizz since data can be explored, annotated, or visualized from any
location or device. Data exchange and upload/download is made easy and
secure via a number of data conversion tools and a user/permission
management system. Furthermore, the system allows real-time
collaboration by means of "shared sessions", where changes in the
visualization are synchronized across multiple users as if they were all
on the same machine.
</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Listening Guides: Ten Year Report</i> - Rodolphe Bailly</div>
          <div>
            This demo is a 10 year report of the production of online Listening Guides by the Cité de la musique and their use inside and outside the institution.
            <br />
            The purpose of the Listening Guides is to explain the language of Music in clear and simple terms, using an online synchronization tool between Music, Text, scores and annotations.
          </div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Music-Related Media Contents Synchronized over the Web: The IEEE 1599 Initiative</i> - Adriano Barat, Stefano Baldan, Davide Andrea Mauro, Goffredo Haus, and Luca Andrea Ludovico</div>
          <i><a href="http://emipiu.di.unimi.it/">http://emipiu.di.unimi.it/</a></i> - <a href="pdf/demo/wac15_submission_2.pdf">PDF</a>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>The Telemeta Platform and TimeSide Framework: Audio Archives Management and Automatic Analysis</i> - Guillaume Pellerin</div>
          <div>
The Telemeta platform and TimeSide framework: Audio archives management
and Automatic analysis.
<br />
Telemeta is an open-source web audio platform for the management and
access to digital sound archives and audio metadata. It is developed by
the Parisson company and other open source developers since 2007.
<br />
Telemeta focuses on the enhanced and collaborative user-experience in
accessing audio items and their associated metadata and on the
possibility for the expert users to further enrich those metadata though
hierarchical and structured fields, thesaurus and ontologies.
<br />
This platform has been deployed since 2011 in the context of
ethnomusicological archives and hold the archives of the Center for
Research in Ethnomusicology, which is the most important ethnic music
collection in Europe. The platform is fully operational and is now used
on a daily basis by researchers, teachers and archivists in the fields
of ethnomusicology, anthropology, linguistics and acoustics.
<br />
The Telemeta audio engine relies on TimeSide, an open audio processing
framework written in Python and JavaScript.
<br />
Not only TimeSide provides Telemeta with audio decoding, encoding and
streaming methods but with a set of on-the-fly signal analysis methods
as well.
<br />
Given these low and high level signal analysis capabilities, various
audio signal representations can be computed and incorporated in the
embedded HTML audio player.
By additionally wrapping several audio features extraction
libraries,various automatic annotation, segmentation and musicological
analysis can be performed over the audio archives and metadata.
<br />
The TimeSide engine architecture is composed of several modules and
makes it easy to develop and add new plugins.
Through collaboration with academic research labs in computer science,
speech processing and music information retrieval, new automatic
analysis functionalities are brought to the platform regularly.
<br />
As component of a fully functional but continuously evolving web
platform, the development of both Telemeta and TimeSide is secured with
unitary testing process.
<br />
The Telemeta and TimeSide platform are available as open-source projects
at the following addresses:
<br />
https://github.com/yomguy/Telemeta<br />
https://github.com/yomguy/TimeSide
          </div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Real-Time Client-Side Physical Modeling Harpsichord</i> - Thomas Cipierre / Laurent Pottier (CIEREC – EA3068)</div>

<div>
As part of the ANR Blanc Project FEEVER <http://www.agence-nationale-recherche.fr/projet-anr/?tx_lwmsuivibilan_pi2%5BCODE%5D=ANR-13-BS02-0008> (ANR-13-BS02-0008, 10/2013, 42 months), the CIEREC <http://portail.univ-st-etienne.fr/bienvenue/recherche/centre-interdisciplinaire-d-etudes-et-de-recherches-sur-l-expression-contemporaine-27653.kjsp> (Saint-Étienne - France) was asked by the Museum of Art and Industry of Saint-Étienne <http://www.musee-art-industrie.saint-etienne.fr/> to implement a real-time synthesized harpsichord for a special exhibition <http://www.musee-art-industrie.saint-etienne.fr/decouvrir/collections/clavecin-du-musee>.
<br />
A standalone version was created by Laurent Pottier and Luc Faure, all DSP edited in FAUST <http://faust.grame.fr/> in a physical modeling approach, with a Max/MSP <https://cycling74.com/> GUI (DSP based on Julius Smith and Romain Michon previous works on the Faust-STK <http://faust.grame.fr/index.php/related-projects/faust-stk>). Our second goal was then to provide a client-side real-time harpsichord on the web, in order to share a convenient musical researching and/or production tool, as much as an educational support to anyone, even with no prior computational and/or musical knowledge.
<br />
The idea was thus to implement a polyphonic Javascript version of our model, thanks to the faust2asmjs script (which consists of compiling our FAUST DSP in an optimized polyphonic asm.js <http://asmjs.org/> version), create instances, use regular Web Audio API nodes, and control DSP parameters through a responsive user-friendly GUI in Polymer <https://www.polymer-project.org/>. Some issues were of course to face, from trying to optimize the amount of computing resources needed by the physical modeling approach (especially for near zero amplitude data), to trying to be the more "web-standard" we could concerning the GUI, despite the unbalanced implementations of W3C requirements through the mainly used web browsers (Web Audio API, Web MIDI API, Web Components).
<br />
You will find our still evolving web harpsichord version here : http://musinf.univ-st-etienne.fr/recherches/ClavecinHtml/web-harpsichord.html
<http://musinf.univ-st-etienne.fr/recherches/ClavecinHtml/harpsi.html>.
<br />
You will also find a quick video demonstration of our first stand-alone version (FAUST DSP - Max/MSP GUI) here : http://feever.fr/videos.
</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Delivering Object-Based 3D Audio using The Web Audio API</i> - Chris Pike</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Binaural Synthesis with the Web Audio API</i> - Thibaut Carpentier - <a href="pdf/demo/wac15_submission_16.pdf">PDF</a></div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Real-Time Acoustic Auralization on the Web</i> - Chinmay Prafulla Pendharkar</div>
          <div>
          Auralization is the technique user for creation and reproduction of
spatially distributed sound based on mathematical models. In recent
times auralization is gaining interest in the field of Room Acoustics as
an effective way to hear how proposed spaces or rooms would distribute
sonic events. Auralization using real-time sound sources takes this
virtual reality one step further by processing the users voice and
reproducing it to give a sense of envelopment similar to the space being
modeled.
<br />
Traditionally most tools for creating these mathematical models, their
outputs in form of impulse responses, or the tools for experiencing
auralization are native apps. Web Audio enables parts of the usecases
for real-time auralization by allowing user audio inputs and fast native
convolution using the ConvolverNode. Earlier this year, I created a demo
of this concept called Auralizr, based on a methodology derived and
implemented in PureData by my Acoustician colleague Josefin Lindebrink.
https://github.com/notthetup/auralizr
<br />
In this talk I'll explain how real-time auralization works on the Web
and especially Mobile Web. I will also discuss some performance in terms
of latency and how things can be improved. I will talk about some use
case ideas and also some future plans we have with this project.
</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Birds of a Feather (Les Oiseaux de Mme Plumage): Dynamic Soundscapes using Real-Time Manipulation of Locally Relevant Birdsongs</i> - Bill Walker and Brian Belet</div>
        </td>
      </tr>
      <tr class="category">
        <td>
          <b>17.00 - 18.30</b>
        </td>
        <td>
          <b>Delivering & Listening - Igor Stravinsky Room</b> | <small>Session Moderator: Thibaut Carpentier</small>
        </td>
      </tr>
      <tr>
        <td>
          17.00
        </td>
        <td>
          <div class="title"><i>Delivering Object-Based 3D Audio using The Web Audio API and The Audio Definition Model</i> - Peter Taylour, Chris Pike, and Frank Melchior - <a href="pdf/wac15_submission_24.pdf">PDF</a></div>
          <div>
            <p>
              Presentation of an application that demonstrates object-based 3D audio rendering in the web browser using the Web Audio API. The application loads audio files containing object-based meta-data and provides head-tracked dynamic binaural rendering of the content to create an immersive 3D audio experience for headphone listeners. The user can interact with the rendering by muting individual audio objects and switching between the binaural rendering mode and conventional stereo rendering.
              The demo also includes visualisations of the object-based meta-data. Active objects are highlights on a 2D timeline visualisation and THREE.js is used to show the rendering location of each audio source in 3D space.
            <br />
              This application demonstrates a future of broadcast sound experiences over the web, where immersive content is rendered on the client and can be adapted to listener context, as page layout is adapted to device context today with responsive design.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          17.20
        </td>
        <td>
          <div class="title"><i>Towards the Next Generation of Web-based Experiments: A Case Study Assessing Basic Audio Quality Following the ITU-R Recommendation BS.1534 (MUSHRA)</i> - Michael Schoeffler, Fabian-Robert St&#353;ter, Bernd Edler, and J&#376;rgen Herre - <a href="pdf/wac15_submission_8.pdf">PDF</a></div>
          <div>
            <p>
              Listening tests are widely used to assess the quality of audio systems. The majority of such listening tests are conducted in controlled environments with selected participants and professional audio equipment. In the last few years, conducting listening tests over the Internet, as so called web-based experiments, has become popular. A recent study has shown that web-based experiments lead to comparable results as laboratory experiments.
            <br />
              Until now, it was only possible to implement a limited number of listening test types as web-based experiments because web standards were missing some crucial features (e.g. sample manipulation of audio streams). With the upcoming of the Web Audio API, a much wider range of listening test types can be implemented as new audio processing features have been introduced. This talk will demonstrate which new possibilities are enabled by the Web Audio API. To this end, the ITU-R Recommendation BS.1534 (MUSHRA) is taken as an example.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          17.40
        </td>
        <td>
          <div class="title"><i>Spatially Distributed Sound Computing and Rendering using the Web Audio Platform</i> - Lonce Wyse - <a href="pdf/wac15_submission_26.pdf">PDF</a></div>
          <div>
            <p>
              Large multi-channel spatial audio systems have historically been a playground for universities and well-funded studios, but only a dream for independent composers.
            <br />
              Similarly, "parallel computers" were locked in research facilities where only a few musicians ever gained access to the computational power to convolve hundreds of separate audio streams with spatially specific room impulse responses. Mobile devices in the hands of audiences can quickly configure themselves into these kinds of systems at very affordable (and distributed) cost and little effort, making powerful and expressive spatially distributed musical platforms accessible to anyone today. We describe some software systems and artistic works that have been recently developed to explore some of the spatial audio capabilities of the mobile device browser platform.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          18.00
        </td>
        <td>
          <div class="title"><i>Personalization Support for Binaural Headphone Reproduction in Web Browsers</i> - Michele Geronazzo, Jari Kleimola, and Piotr Majdak - <a href="pdf/wac15_submission_29.pdf">PDF</a></div>
          <div>
            <p>
              This study considers the issue of providing an individual listening experience for binaural sound reproduction in web browsers via headphones. The proposed solution aims at building a web framework with Web Audio API, giving support to the download of head-related transfer functions (HRTFs) associated with listener's personal profile from a server and the synchronization between the listener's devices. With each playback device and listener, the individual headphone equalization filters will be computed from headphone transfer functions (HpTFs) stored on the server. At server side, we propose to store the HRTFs and HpTFs in spatially oriented format for acoustics (SOFA). At client-side, we propose to convert the data to a new structure (WAV) ensuring a compatible solution with existing Web Audio API implementations. A binaural rendering implementation in JavaScript acting as a proof-of-concept reveals critical issues related to the native implementation in web browsers.
            </p>
          </div>
        </td>
      </tr>
    </table>
    <table>
      <tr class="day">
        <th colspan=2>Tuesday morning, January 27, 2015 - IRCAM - Igor Stravinsky Room</th>
      </tr>
      <tr>
        <td>
          9.15
        </td>
        <td>
          <div class="title"><i>Keynote #3 - Web Audio API vs. Native: Closing the Gap</i> - Paul Adenot</div>
          <div>
            <p>
              Audio is one of the domains where developers try to get every bit of
              performance out of the device. On the other hand, the Web Audio API
              looks like an high-level API with a lot of constraints for developers.
              What does the web platform need for the Web Audio API to be competitive
              with native audio? What problems does the platform have that can be
              solved today?
            <br />
              Paul is an audio developer at Mozilla, working on the Firefox web
              browser. He works on the Firefox Web Audio implementation, as well as
              the platform-specific audio code on all platforms, and WebRTC. He also
              co-edits the Web Audio API specification at the W3C, and is a long time
              guitar player.
            </p>
          </div>
        </td>
      </tr>
      <tr class="category">
        <td>
          10.00
        </td>
        <td>
          Coffee Break (Gallery, Level-2)
        </td>
      </tr>
      <tr class="category">
        <td>
          <b>10.30 - 13.00</b>
        </td>
        <td>
          <b>Applications</b> |  <small>Session Moderator: Yann Orlarey</small>
        </td>
      </tr>
      <tr>
        <td>
          10.30
        </td>
        <td>
          <div class="title"><i>Lissajous: Performing Music with Javascript</i> - Kyle Stetz</div>
          <div>
            <p>Lissajous is a live coding instrument designed for use in the developer console. Its concise and functional API allows you to create dynamic sounds and rhythms in a single line of code.
              <a href="http://lissajousjs.com">http://lissajousjs.com</a>
              <a href="https://github.com/kylestetz/lissajous">https://github.com/kylestetz/lissajous</a>
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          10.50
        </td>
        <td>
          <div class="title"><i>EarSketch: Teaching Computational Music Remixing in an Online Web Audio Based Learning Environment</i> - Anand Mahadevan, Jason Freeman, Brian Magerko, and Juan Carlos Martinez - <a href="pdf/wac15_submission_3.pdf">PDF</a></div>
          <div>
            <p>
              EarSketch is a novel approach to teaching computer-science concepts via algorithmic music composition and remixing in the context of a digital audio workstation paradigm. This project includes a Python/JavaScript coding environment, a digital audio workstation view, an audio loop browser, a social sharing site and an integrated curriculum. EarSketch is aimed at satisfying both artistic and pedagogical goals of introductory courses in computer music and computer science. This integrated platform has proven particularly effective at engaging culturally and economically diverse students in computing through music creation. EarSketch makes use of the Web Audio API as its primary audio engine for playback, effects processing and online rendering of audio data. This presentation explores the technical framework of EarSketch in greater detail and discusses the opportunities and challenges associated with using the Web Audio API to realize the project.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          11.10
        </td>
        <td>
          <div class="title"><i>Hyperaudio</i> - Mark Boas</div>
          <div>
            <p>
              Audio is often placed on the Internet without much thought about how it can be found, searched, shared, navigated and generally deconstructed.
              Hyperaudio weaves audio into the very fabric of the web. By transcribing the spoken parts of audio and assigning timings to each an every word we allow people to navigate, search and share in naturally and accurately manner. Uniquely Hyperaudio allows people to intuitively edit audiovisual content from its transcript.
              Editing media content with Hyperaudio is as easy as editing a text document.
            <br />
              The Hyperaud.io service allows people to link, to transcribe and align media, and then remix it with the Hyperaudio Pad - augmenting excerpts of audio and video with effects and music.
              We're currently working with libraries and schools and using Hyperaudio to foster media literacy in a new generation.
              In order to create hyperaud.io we built an API, a series of decoupled tools and our MIT licensed JavaScript Library - Hyperaudio.js - which anyone can use to open up their audio.
              <br>
              <a href="http://hyperaud.io">http://hyperaud.io</a>
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          11.30
        </td>
        <td>
          <div class="title"><i>Birds of a Feather (Les Oiseaux de Mme Plumage): Dynamic Soundscapes using Real-Time Manipulation of Locally Relevant Birdsongs</i> - Bill Walker and Brian Belet - <a href="pdf/wac15_submission_22.pdf">PDF</a></div>
          <div>
            <p>
              This presentation and live audio demonstration explores the capabilities of using the Web Audio API as a digital audio workstation (DAW) to manipulate sounds from massive server-side databases. Sonic source material comes from a database of birdsongs recorded worldwide by volunteer recordists at xeno-canto.org. Sounds from xeno-canto are chosen to match recent, nearby bird sightings submitted by volunteer birders at eBird. The result is a virtual soundscape derived from the sounds of birds currently present in the user’s physical environment.
            <br />
              Our software delegates database queries and archival storage to the server, leaving the client to concentrate on the aesthetic context of sound modification and manipulation. Engineering issues include separation of client versus server concerns and mashups of crowdsourced databases. Aesthetic issues include which tasks are automated server-side, which are user-controlled client-side, and why. Social issues include single user versus multiple user paradigms, artistic soundscape composition versus commercial applications (e.g., games with evolving sound tracks) using public domain sound sources, music as foreground art versus background audio content, and the larger role of sound and music in current society. Audio results will be demonstrated as each topic is addressed.
            <br />
              All the source code for this project is free available under the MIT License.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          11.50
        </td>
        <td>
          <div class="title"><i>VenueExplorer, Object-Based Interactive Audio for Live Events</i> - Matthew Paradis, Rebecca Gregory-Clarke, and Frank Melchior - <a href="pdf/wac15_submission_7.pdf">PDF</a></div>
          <div>
            <p>
              VenueExplorer is a new approach to broadcasting live events which gives more control to the audience than traditional viewing methods. Users can navigate around an ultra-high resolution video, zooming into the areas of the event which interest them and accessing extra content. VenueExplorer aims to be platform independent and runs in the browser. In this paper we describe the development of object-based audio rendering to create a more engaging and personalised experience than that of video alone. We use the Web Audio API to process audio based on the users viewport. We also describe a library that has been developed as part of this project for the handling of location based audio objects.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          12.10
        </td>
        <td>
          <div class="title"><i>Noteflight: A Web-standards-based Compositional Community</i> - Joseph Berkovitz</div>
          <div>
            <p>Noteflight is a web application and vibrant online community that supports the creation, sharing and playback of scores using conventional Western music notation, all taking place within a standard web browser. The site has been live for 6 years and has over 1.2 million registered users today. Of necessity, Noteflight was launched using the Adobe Flash platform, but in the last several years it has successfully transitioned to a pure Web standards environment including SVG and Web Audio.            <br />
              Prior to Noteflight, interactive notation editing was only available via a small number of native desktop applications. This state of affairs kept composers, arrangers and musicians from participating in the revolution in communication that has so changed human affairs in recent decades. Today, Noteflight provides a planetary-scale community for musical creation, consumption and education that is both free and standards-based.
            <br />
              The most musically significant and innovative features of Noteflight include the notation editor and its built-in sequencer, synthesizer and mixer that use downloadable instrument samples. This talk will discuss significant implementation challenges for these components, and look at how they differ from analogous components in the native-app arena. Particular attention will be given to how being a networked, community-based application affected many design goals. Along the way, the talk will also shed light on the challenges facing Noteflight's transition from a proprietary platform (Flash) to a pure web-standards platform.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          12.30
        </td>
        <td>
          <div class="title"><i>Music Performance by Discovering Community Loops</i> - Gerard Roma and Xavier Serra - <a href="pdf/wac15_submission_39.pdf">PDF</a></div>
          <div>
            <p>
              Technologies for discovering sounds in large databases can help blurring the boundary between exploration and music performance. In this paper, we present a system for exploring loops from Freesound.org. Sound files are grouped by their most common repetition periods, so that they can be played in sync. A graph layout algorithm is used to organize sounds in a two-dimensional plane so that loops with similar timbre are spatially close. The result is a system that can be used as a musical instrument: since sounds will always play in sync, the user can freely explore the variety of sounds uploaded by the Freesound community, while continuously producing a rhythmic musical stream.
            </p>
          </div>
        </td>
      </tr>
      <tr class="category">
        <td>
          13.00
        </td>
        <td>
          Buffet
        </td>
      </tr>
    </table>
    <table>
      <tr class="day" >
        <th colspan=2>
          Tuesday afternoon, January 27, 2015 - MOZILLA
        </th>
      </tr>
      <tr class="category">
        <td>
          <b>15.00</b>
        </td>
        <td>
          <b>Demo/Poster Session #2</b>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>The Collective Sound Checks Mobile Web Audio Applications</i> - Norbert Schnell</div>
          <div>
          We demonstrate a set of Web Audio API based prototype applications that
have been developed to explore scenarios of collective performance with
sound and music in the context of the CoSiMa project
(http://cosima.ircam.fr/).<br />
The applications feature various synthesis techniques implemented with
the Web Audio API that are controlled through the embedded sensors and
touch screen of mobile devices (i.e. smartphones).
Since the applications focus on motion-based control, their graphical
user interfaces are limited to a few buttons to select different options.
<br />
We have tested different performance scenarios based on these
applications in a series of workshops – the ”Collective Sound Checks” –
in collaboration with the Studio 13/16 at the Centre Pompidou in Paris.
Three workshops were held in May and June 2014, and four further ones
between October and December 2014.
<br />
We have published some of the applications which only require a simple
HTTP server at http://cosima.ircam.fr/checks/.
The other applications provide an infrastructure to the performers that
relies on a node.js server and thus require a local server and WiFi
network for the demonstration.
<br />
More information about the applications – including videos showing their
functioning – can be found on the CoSiMa web page at
- http://cosima.ircam.fr/2014/07/15/cosc-web-applications<br />
- http://cosima.ircam.fr/2014/07/15/cosc-shaker/<br />
- http://cosima.ircam.fr/2014/07/15/cosc-wwryr/
</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Soundworks – A Playground for Artists and Developers to Create Collaborative Mobile Web Performances</i> - Sébastien Robaszkiewicz and Norbert Schnell - <a href="pdf/demo/wac15_submission_30.pdf">PDF</a></div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Humming Mississippi</i> - Jesse Allison</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Scrolling Through Sound</i> - Ehsan Ziya</div>
          <i
            ><a href="http://zya.github.io/scrollsound/">http://zya.github.io/scrollsound/</a> - <a href="pdf/demo/wac15_submission_15.pdf">PDF</a></i>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Web Audio Synthesizer Design</i> - Luke Teaford</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Two Online N-gon Wave Synthesisers</i> - Dominik Chapman</div>
          <div>
N-gon wave synthesis and especially n-gon wave scales are relatively unknown to the public. The approach to this synthesis and composition is partially inspired from experimental approaches to drawn graphical / ornamental sound, that include methods that came to prominence in Russia and Germany in the first part of the 20th century. Two online synthesisers (current link: http://igor.ac.uk/~mu102dc/ngonwaves/start.html) were developed at Goldsmiths University of London as introductory tools to the subject of geometric waveforms. The online real-time interaction tools are a means to explore the audio-visual relations of the geometry of waveforms that are derived from regular polygons and regular star polygons and allow to experiment with a theoretically infinite number of geometric waveforms and scales. The poster presents a short introduction to n-gon wave synthesis, the two n-gon wave synthesisers and a critical evaluation of the n-gon wave algorithm implementation using the Web Audio API and the HTML canvas element.
</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>LFO Low Frequency Operators on Streams</i> - Victor Saiz</div>
          <div>
LFO is an API that aims to formalise the processing and analysis of
arbitrary data streams (audio, video, sensor data etc.) . By normalising
the stream format in it's input and output ends we will be able to
manipulate and analyse the data through a processing chain and
encapsulate common processing algorithms with a unified interface that
can be shared and reused.
<br />
For such goal we will explore javascript possibilities from browser APIs
such as workers to node's server-side streaming capabilities.
<br />
We will then present LFO as an architecture pattern for control signal
processing (eg. parameter control of an audio node via LFO signals) and
signal analysis and automatic description (onset detection, automatic
description and segmentation etc.)
          </div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Visualizing Audio with p5.js</i> - Jason Sigal</div>
<div>
p5.js is a new JavaScript library from the Processing Foundation. It begins
with the original goal of Processing—to make programming accessible for
artists, designers, educators and novices—and re-imagines this mission for
the web. p5.js also has addon libraries that make it easy to interact with
other HTML5 objects, including text, input, video, webcam, and sound.
<br />
This demo showcases p5's methodologies for welcoming creative coders of all
stripes to the exciting world of Web Audio. The core sound library is
p5.sound. It is designed to be accessible to those with no code or sound
background, while also fostering the creativity of more advanced users. In
addition, p5 welcomes contributed addon libraries. For example, p5.gibber
is a wrapper for the audio portion of the Gibber coding environment,
created by Charlie Roberts. More information at p5js.org/libraries.
</div>

        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Quint.js: A JavaScript Library for Teaching Music Technology to Fine Arts Students</i> - Ian George Burleigh and Thilo Schaller</div>
          <i><a href="http://quinta.audio/Quint">http://quinta.audio/Quint</a></i> - <a href="pdf/demo/wac15_submission_20.pdf">PDF</a>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>A Dynamic Audio Experience Creation Platform in Web Audio</i> - Chinmay Pendharkar, Peter Bäck, and Lonce Wyse</div>
          <i><a href="http://wac.sonoport.com/">http://wac.sonoport.com/</a></i> - <a href="pdf/demo/wac15_submission_13.pdf">PDF</a>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Websocket Server for Max</i> - Oliver Larkin</div>
<div>
This demo introduces ol.wsserver, an open source Max external that uses HTML5 WebSockets to provide a simple solution for multi-client interaction with a Max patch. The external embeds the Civetweb web server and allows the server side Max patch to send and receive data to/from individual WebSocket clients via Max messages. It is primarily designed for WLAN based performances and sound installations involving mobile phones and tablets, although it should also work over the internet. The use of standard HTTP ports and an embedded server reduces the complexity of setup when compared to alternatives such as dedicated OSC controller apps.

</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Streaming Live Content to Web Audio API</i> - Raphaël Goldwaser and Emmanuel Fréard - <a href="pdf/demo/wac15_submission_35.pdf">PDF</a></div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>VenueExplorer (Demo) Object-Based Interactive Audio for Live Events</i> - Matthew Paradis</div>
<div>
VenueExplorer is a new approach to broadcasting live events which gives
more control to the audience than traditional viewing methods. Users can
navigate around an ultra-high resolution video, zooming into the areas
of the event which interest them and accessing extra content.
VenueExplorer aims to be platform independent and runs in the browser.
We use the Web Audio API to process audio based on the users viewport.
This allows relevant audio to be presented according to the event that
the user is watching.
</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>MT5: a HTML5 Multitrack Player for Musicians</i> - Michel Buffa, Amine Hallili and Philippe Renevier - <a href="pdf/demo/wac15_submission_18.pdf">PDF</a></div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Adaptive, Personalised "In Browser" Audio Compression </i> - Matthew Paradis and Andrew Mason - <a href="pdf/demo/wac15_submission_25.pdf">PDF</a></div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Seismokraft</i> - Ethan Geller</div>
<div>
          Seismokraft (developed with Elizabeth Davis) is a web application that
pulls time series data from recent seismic events and allows the user to
manipulate the resulting audio signal using the webAudio API. The
ultimate goal is to allow users to compare and contrast properties of
different seismic events. http://seismokraft.com
</div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>SimScene: a Web-based Acoustic Scenes Simulator</i> - Mathias Rossignol, Gregoire Lafay, Mathieu Lagrange, and Nicolas Misdariis - <a href="pdf/demo/wac15_submission_31.pdf">PDF</a></div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Querying Freesound with a Microphone</i> - Gerard Roma and Xavier Serra - <a href="pdf/demo/wac15_submission_37.pdf">PDF</a></div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div><i>Composing a Web of Audio Applications</i> - Sarah Denoux, Yann Orlarey, Stephane Letz, and Dominique Fober - <a href="pdf/demo/wac15_submission_32.pdf">PDF</a></div>
        </td>
      </tr>
      <tr class="category">
        <td>
          <b>17.00 </b>
        </td>
        <td>
          <b>Web Audio Gigs #1</b>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>The Tomb of the Grammarian Lysias</i> - Ben Houge</div>
          <div>
Before his death in 1933, Constantine P. Cavafy was a poet on the fringe, living on the outskirts of the Greek diaspora in Alexandria, writing in a modernist style that was far from the mainstream of his time, and marginalized for his homosexuality.  In this poem, he describes the tomb of the fictional scholar Lysias, and the stochastic shuffling of the electronic accompaniment, distributed throughout the audience, reflects the way we explore space and acquire knowledge, eyes falling where they may, moving from one subject to the next.  We might peruse the stacks of a library the same way, and Cavafy’s oeuvre is notable for finding unexpected correspondences between the annals of Hellenistic history and our own time.  This setting is in just intonation, which connects Cavafy to another prominent Greek from Alexandria, Ptolemy, who first theorized a musical system based on small number frequency ratios.  In its non-tempered tunings, phrasings, and drone-based textures, the music evokes Greek Orthodox chant, and its generative musical processes, which could continue indefinitely, create a somber space for reflection, fitting for a memorial.
          </div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Traversal</i> - Jesse Allison</div>
          <div>
Traversal is a series of performances for physical instruments, collaborators in virtual space, and computer mediator.  This performance emphasizes a collaborative performance by the audience on web enabled devices.  The interface and interactions are a series of spaces that connect audience members with each other and in doing so, create sonic events in the space and on the user's device.  Interactions include touching other audience participants through the virtual space, passing sound events from one user to another, and playing individual musical ideas that weave together into a sonic tapestry across the audience.
          </div>
        </td>
      </tr>
      <tr class="category">
        <td>
          18.00
        </td>
        <td>
          Happy Hour
        </td>
      </tr>
      <tr class="category">
        <td>
          <b>19.00</b>
        </td>
        <td>
          <b>Web Audio Gigs #2</b>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Drops</i> - Sébastien Robaszkiewicz and Norbert Schnell</div>
          <div>
          The Drops collective smartphone performance is strongly inspired by the mobile application Bloom by Brian Eno and and Peter Chilvers. Drops reproduces several audiovisual elements of the original Bloom application while transposing them into a collaborative experience.
          <br />
          In Drops, each participant can only play a single sound (i.e. a single pitch), whose timbre can vary depending on the touch position. Together, the players can construct sound sequences (i.e. melodies) by combining their sounds. The sounds are repeated in a fading loop every few seconds until they vanish. Players can clear the loop by shaking their smartphones. The sounds triggered by one player are automatically echoed by the smartphones of other players. The collective performance on the smartphones is accompanied by a synchronized soundscape on ambient loudspeakers.
          </div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Smartphone Jam Session with Audience</i> - Toshihiro Kita</div>
          <div>
            This piece is produced live by interaction between a laptop connected to the
smartphones of the audience. Simultaneously up to 16 people can join the
interaction. <br />
The first part is made of many simple decaying tones and the audience can fluctuate
the frequencies and the depth of reverb by touching their smartphones. The second
part they can modulate the degree of distortion and the panning position of a slowly
waving cluster of tones by tilting and moving the smartphones around.
The third (last) part has nothing until each of audience touches his/her smartphone.
Every time each person touches, a tone is generated, and gradually changes according
to how many times they touch. <br />
The system configuration is similar to one explained in
<a href="http://www.jssa.info/paper/2013v05n03/PDF/3.Kita.pdf">http://www.jssa.info/paper/2013v05n03/PDF/3.Kita.pdf</a> (in Japanese). Similarly
configured pieces were played at 2nd International Csound Conference (
<a href="http://www.youtube.com/watch?v=J12iOcZkI6I">http://www.youtube.com/watch?v=J12iOcZkI6I</a> ). This time client-side audio processing
by HTML5 Web Audio API is added to enhance feedback to each participant. The files
to be used will be put at <a href="http://tkita.net/csound/wac2014/">http://tkita.net/csound/wac2014/</a> .
          </div>
        </td>
      </tr>
      <tr class="category">
        <td>
          20.00
        </td>
        <td>
          Buffet
        </td>
      </tr>
      <tr class="category">
        <td>
          <b>21.00</b>
        </td>
        <td>
          <b>Web Audio Gigs #3</b>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Pearl River</i> (2013/2015) - Benjamin Taylor</div>
          <div>
Pearl River is a distributed audio work in which many participants act as parts of a single instrument. In 2013, 10-20 networked laptop performers performed the first iteration of the piece, in which each performer controls one sine tone in a spectrum, and the ensemble organizes chords via a chat window and networked visual interface. For the Web Audio Conference 2015, Pearl River has been adapted to take advantage of a larger audience of participants (50+) on mobile phones. Several musical interfaces have been added to the original and are distributed to the audience. As new interfaces are sent to the audience, participants act as parts of a collective piano, parts of a collective granular synth, and parts of an overtone spectrum.
          </div>
        </td>
      </tr>
      <tr>
        <td class="no-border"></td>
        <td>
          <div class="title"><i>Fields #2</i> - Sébastien Piquemal and Tim Shaw</div>
          <div>Sébastien and Tim explore mobile technology as a medium for sound diffusion.
            <br />
Audience members can join in by simply connecting to a specific website with their mobile phone, laptop or tablet. The connected devices become an array of speakers that the performers can control live, resulting in an omni-directional sonic experience.
          </div>
        </td>
      </tr>
      </table>
    <table>
      <tr class="day">
        <th colspan=3 >
         Wednesday morning, January 28, 2015 - IRCAM - Igor Stravinsky room
        </th>
      </tr>
      <tr>
        <td>
          9.30
        </td>
        <td>
          Coffee
        </td>
      </tr>
      <tr>
        <td>
          10.00
        </td>
        <td>
          <div class="title"><i>W3C Audio Working Group Plenary Session</i> - Matthew Paradis, Joe Berkovitz, Chris Lowis, Paul Adenot, and Chris Lilley</div>
          <div>
            <p>
              With Matthew Paradis (Chair), Joe Berkovitz (Chair), Chris Lowis (Invited Expert), Paul Adenot (Editor/Implementor), Chris Lilley (W3C Staff Representative)
            <br />
              A number of members of the W3C Web Audio Group will be present at the WAC conference. The Audio Working Group will give a short presentation on how the process works from the W3C perspective and will highlight some of the areas that the Web audio community could help in the effort to move the API to a W3C recommendation. The Audio Working Group will then present some of the more complex issues that it deals with for discussion and take questions and suggestions from attendees.
            </p>
          </div>
        </td>
      </tr>
      <tr>
        <td>
          12.00
        </td>
        <td>
          Free Time
        </td>
      </tr>
    </table>
    <table border="1">
      <tr class="day">
        <th colspan=2>
         Wednesday afternoon, January 28, 2015 - Mozilla
        </th>
      </tr>
      <tr>
        <td>
          14.00 - 18.30
        </td>
        <td>
          Experiments, Hacks, Informal
          Presentations, and Discussions
        </td>
      </tr>
    </table>
  </body>
</html>
